program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3510.2.1"}, {"coremlc-version", "3500.32.1"}, {"coremltools-component-torch", "2.7.0"}, {"coremltools-source-dialect", "TorchExport::ATEN"}, {"coremltools-version", "9.0"}})]
{
    func main<ios16>(tensor<int32, [1, 128]> attention_mask, tensor<int32, [1, 128]> input_ids) {
            tensor<int32, [1]> unsqueeze_axes_0 = const()[name = tensor<string, []>("unsqueeze_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 1, 128]> unsqueeze = expand_dims(axes = unsqueeze_axes_0, x = attention_mask)[name = tensor<string, []>("unsqueeze")];
            tensor<int32, [1]> unsqueeze_1_axes_0 = const()[name = tensor<string, []>("unsqueeze_1_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, 128]> unsqueeze_1 = expand_dims(axes = unsqueeze_1_axes_0, x = unsqueeze)[name = tensor<string, []>("unsqueeze_1")];
            tensor<fp16, []> const_17_to_fp16 = const()[name = tensor<string, []>("const_17_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<string, []> _to_copy_to_fp16_dtype_0 = const()[name = tensor<string, []>("_to_copy_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 1, 128]> unsqueeze_1_to_fp16 = cast(dtype = _to_copy_to_fp16_dtype_0, x = unsqueeze_1)[name = tensor<string, []>("cast_45")];
            tensor<fp16, [1, 1, 1, 128]> rsub_cast_fp16 = sub(x = const_17_to_fp16, y = unsqueeze_1_to_fp16)[name = tensor<string, []>("rsub_cast_fp16")];
            tensor<fp16, []> const_18_to_fp16 = const()[name = tensor<string, []>("const_18_to_fp16"), val = tensor<fp16, []>(-inf)];
            tensor<fp16, [1, 1, 1, 128]> mul_cast_fp16 = mul(x = rsub_cast_fp16, y = const_18_to_fp16)[name = tensor<string, []>("mul_cast_fp16")];
            tensor<int32, []> embedding_axis_0 = const()[name = tensor<string, []>("embedding_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> embedding_batch_dims_0 = const()[name = tensor<string, []>("embedding_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [30522, 384]> p_transformer_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [30522, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<fp16, [1, 128, 384]> embedding_cast_fp16 = gather(axis = embedding_axis_0, batch_dims = embedding_batch_dims_0, indices = input_ids, x = p_transformer_embeddings_word_embeddings_weight_to_fp16)[name = tensor<string, []>("embedding_cast_fp16")];
            tensor<fp16, [1, 128, 384]> embedding_1_to_fp16 = const()[name = tensor<string, []>("embedding_1_to_fp16"), val = tensor<fp16, [1, 128, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23441024)))];
            tensor<fp16, [1, 128, 384]> add_cast_fp16 = add(x = embedding_cast_fp16, y = embedding_1_to_fp16)[name = tensor<string, []>("add_cast_fp16")];
            tensor<fp16, [1, 128, 384]> embedding_2_to_fp16 = const()[name = tensor<string, []>("embedding_2_to_fp16"), val = tensor<fp16, [1, 128, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23539392)))];
            tensor<fp16, [1, 128, 384]> add_1_cast_fp16 = add(x = add_cast_fp16, y = embedding_2_to_fp16)[name = tensor<string, []>("add_1_cast_fp16")];
            tensor<int32, [1]> layer_norm_axes_0 = const()[name = tensor<string, []>("layer_norm_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_embeddings_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_embeddings_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23637760)))];
            tensor<fp16, [384]> p_transformer_embeddings_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_embeddings_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23638592)))];
            tensor<fp16, []> const_21_to_fp16 = const()[name = tensor<string, []>("const_21_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_cast_fp16 = layer_norm(axes = layer_norm_axes_0, beta = p_transformer_embeddings_layernorm_bias_to_fp16, epsilon = const_21_to_fp16, gamma = p_transformer_embeddings_layernorm_weight_to_fp16, x = add_1_cast_fp16)[name = tensor<string, []>("layer_norm_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23639424)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23934400)))];
            tensor<fp16, [1, 128, 384]> linear_0_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_0_attention_self_query_weight_to_fp16, x = layer_norm_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23935232)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24230208)))];
            tensor<fp16, [1, 128, 384]> linear_1_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_0_attention_self_key_weight_to_fp16, x = layer_norm_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<int32, [4]> const_22 = const()[name = tensor<string, []>("const_22"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_cast_fp16 = reshape(shape = const_22, x = linear_1_cast_fp16)[name = tensor<string, []>("view_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24231040)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24526016)))];
            tensor<fp16, [1, 128, 384]> linear_2_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_0_attention_self_value_weight_to_fp16, x = layer_norm_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<int32, [4]> const_24 = const()[name = tensor<string, []>("const_24"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_1_cast_fp16 = reshape(shape = const_24, x = linear_2_cast_fp16)[name = tensor<string, []>("view_1_cast_fp16")];
            tensor<int32, [4]> const_25 = const()[name = tensor<string, []>("const_25"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_26 = const()[name = tensor<string, []>("const_26"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_2_cast_fp16 = reshape(shape = const_26, x = linear_0_cast_fp16)[name = tensor<string, []>("view_2_cast_fp16")];
            tensor<bool, []> matmul_transpose_x_0 = const()[name = tensor<string, []>("matmul_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_transpose_y_0 = const()[name = tensor<string, []>("matmul_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_18_perm_0 = const()[name = tensor<string, []>("transpose_18_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_19_perm_0 = const()[name = tensor<string, []>("transpose_19_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_19 = transpose(perm = transpose_19_perm_0, x = view_cast_fp16)[name = tensor<string, []>("transpose_51")];
            tensor<fp16, [1, 12, 128, 32]> transpose_18 = transpose(perm = transpose_18_perm_0, x = view_2_cast_fp16)[name = tensor<string, []>("transpose_52")];
            tensor<fp16, [1, 12, 128, 128]> matmul_cast_fp16 = matmul(transpose_x = matmul_transpose_x_0, transpose_y = matmul_transpose_y_0, x = transpose_18, y = transpose_19)[name = tensor<string, []>("matmul_cast_fp16")];
            tensor<fp16, []> _inversed_div_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_cast_fp16 = mul(x = matmul_cast_fp16, y = _inversed_div_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_2_cast_fp16 = add(x = _inversed_div_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_2_cast_fp16")];
            tensor<int32, []> const_31 = const()[name = tensor<string, []>("const_31"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_cast_fp16 = softmax(axis = const_31, x = add_2_cast_fp16)[name = tensor<string, []>("softmax_cast_fp16")];
            tensor<bool, []> matmul_1_transpose_x_0 = const()[name = tensor<string, []>("matmul_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_1_transpose_y_0 = const()[name = tensor<string, []>("matmul_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_1_cast_fp16 = transpose(perm = const_25, x = view_1_cast_fp16)[name = tensor<string, []>("transpose_53")];
            tensor<fp16, [1, 12, 128, 32]> matmul_1_cast_fp16 = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = softmax_cast_fp16, y = permute_1_cast_fp16)[name = tensor<string, []>("matmul_1_cast_fp16")];
            tensor<int32, [4]> const_32 = const()[name = tensor<string, []>("const_32"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_33 = const()[name = tensor<string, []>("const_33"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_3_cast_fp16 = transpose(perm = const_32, x = matmul_1_cast_fp16)[name = tensor<string, []>("transpose_50")];
            tensor<fp16, [1, 128, 384]> view_3_cast_fp16 = reshape(shape = const_33, x = permute_3_cast_fp16)[name = tensor<string, []>("view_3_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24526848)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24821824)))];
            tensor<fp16, [1, 128, 384]> linear_3_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_0_attention_output_dense_weight_to_fp16, x = view_3_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_3_cast_fp16 = add(x = linear_3_cast_fp16, y = layer_norm_cast_fp16)[name = tensor<string, []>("add_3_cast_fp16")];
            tensor<int32, [1]> layer_norm_1_axes_0 = const()[name = tensor<string, []>("layer_norm_1_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24822656)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24823488)))];
            tensor<fp16, []> const_35_to_fp16 = const()[name = tensor<string, []>("const_35_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_1_cast_fp16 = layer_norm(axes = layer_norm_1_axes_0, beta = p_transformer_encoder_layer_0_attention_output_layernorm_bias_to_fp16, epsilon = const_35_to_fp16, gamma = p_transformer_encoder_layer_0_attention_output_layernorm_weight_to_fp16, x = add_3_cast_fp16)[name = tensor<string, []>("layer_norm_1_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24824320)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26004032)))];
            tensor<fp16, [1, 128, 1536]> linear_4_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_0_intermediate_dense_weight_to_fp16, x = layer_norm_1_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<string, []> gelu_mode_0 = const()[name = tensor<string, []>("gelu_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_cast_fp16 = gelu(mode = gelu_mode_0, x = linear_4_cast_fp16)[name = tensor<string, []>("gelu_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26007168)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27186880)))];
            tensor<fp16, [1, 128, 384]> linear_5_cast_fp16 = linear(bias = p_transformer_encoder_layer_0_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_0_output_dense_weight_to_fp16, x = gelu_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_4_cast_fp16 = add(x = linear_5_cast_fp16, y = layer_norm_1_cast_fp16)[name = tensor<string, []>("add_4_cast_fp16")];
            tensor<int32, [1]> layer_norm_2_axes_0 = const()[name = tensor<string, []>("layer_norm_2_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27187712)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_0_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_0_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27188544)))];
            tensor<fp16, []> const_37_to_fp16 = const()[name = tensor<string, []>("const_37_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_2_cast_fp16 = layer_norm(axes = layer_norm_2_axes_0, beta = p_transformer_encoder_layer_0_output_layernorm_bias_to_fp16, epsilon = const_37_to_fp16, gamma = p_transformer_encoder_layer_0_output_layernorm_weight_to_fp16, x = add_4_cast_fp16)[name = tensor<string, []>("layer_norm_2_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27189376)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27484352)))];
            tensor<fp16, [1, 128, 384]> linear_6_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_1_attention_self_query_weight_to_fp16, x = layer_norm_2_cast_fp16)[name = tensor<string, []>("linear_6_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27485184)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27780160)))];
            tensor<fp16, [1, 128, 384]> linear_7_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_1_attention_self_key_weight_to_fp16, x = layer_norm_2_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<int32, [4]> const_38 = const()[name = tensor<string, []>("const_38"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_4_cast_fp16 = reshape(shape = const_38, x = linear_7_cast_fp16)[name = tensor<string, []>("view_4_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27780992)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28075968)))];
            tensor<fp16, [1, 128, 384]> linear_8_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_1_attention_self_value_weight_to_fp16, x = layer_norm_2_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<int32, [4]> const_40 = const()[name = tensor<string, []>("const_40"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_5_cast_fp16 = reshape(shape = const_40, x = linear_8_cast_fp16)[name = tensor<string, []>("view_5_cast_fp16")];
            tensor<int32, [4]> const_41 = const()[name = tensor<string, []>("const_41"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_42 = const()[name = tensor<string, []>("const_42"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_6_cast_fp16 = reshape(shape = const_42, x = linear_6_cast_fp16)[name = tensor<string, []>("view_6_cast_fp16")];
            tensor<bool, []> matmul_2_transpose_x_0 = const()[name = tensor<string, []>("matmul_2_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_2_transpose_y_0 = const()[name = tensor<string, []>("matmul_2_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_20_perm_0 = const()[name = tensor<string, []>("transpose_20_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_21_perm_0 = const()[name = tensor<string, []>("transpose_21_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_21 = transpose(perm = transpose_21_perm_0, x = view_4_cast_fp16)[name = tensor<string, []>("transpose_47")];
            tensor<fp16, [1, 12, 128, 32]> transpose_20 = transpose(perm = transpose_20_perm_0, x = view_6_cast_fp16)[name = tensor<string, []>("transpose_48")];
            tensor<fp16, [1, 12, 128, 128]> matmul_2_cast_fp16 = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = transpose_20, y = transpose_21)[name = tensor<string, []>("matmul_2_cast_fp16")];
            tensor<fp16, []> _inversed_div_1_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_1_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_1_cast_fp16 = mul(x = matmul_2_cast_fp16, y = _inversed_div_1_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_1_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_5_cast_fp16 = add(x = _inversed_div_1_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_5_cast_fp16")];
            tensor<int32, []> const_47 = const()[name = tensor<string, []>("const_47"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_1_cast_fp16 = softmax(axis = const_47, x = add_5_cast_fp16)[name = tensor<string, []>("softmax_1_cast_fp16")];
            tensor<bool, []> matmul_3_transpose_x_0 = const()[name = tensor<string, []>("matmul_3_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_3_transpose_y_0 = const()[name = tensor<string, []>("matmul_3_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_5_cast_fp16 = transpose(perm = const_41, x = view_5_cast_fp16)[name = tensor<string, []>("transpose_49")];
            tensor<fp16, [1, 12, 128, 32]> matmul_3_cast_fp16 = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = softmax_1_cast_fp16, y = permute_5_cast_fp16)[name = tensor<string, []>("matmul_3_cast_fp16")];
            tensor<int32, [4]> const_48 = const()[name = tensor<string, []>("const_48"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_49 = const()[name = tensor<string, []>("const_49"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_7_cast_fp16 = transpose(perm = const_48, x = matmul_3_cast_fp16)[name = tensor<string, []>("transpose_46")];
            tensor<fp16, [1, 128, 384]> view_7_cast_fp16 = reshape(shape = const_49, x = permute_7_cast_fp16)[name = tensor<string, []>("view_7_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28076800)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28371776)))];
            tensor<fp16, [1, 128, 384]> linear_9_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_1_attention_output_dense_weight_to_fp16, x = view_7_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_6_cast_fp16 = add(x = linear_9_cast_fp16, y = layer_norm_2_cast_fp16)[name = tensor<string, []>("add_6_cast_fp16")];
            tensor<int32, [1]> layer_norm_3_axes_0 = const()[name = tensor<string, []>("layer_norm_3_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28372608)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28373440)))];
            tensor<fp16, []> const_51_to_fp16 = const()[name = tensor<string, []>("const_51_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_3_cast_fp16 = layer_norm(axes = layer_norm_3_axes_0, beta = p_transformer_encoder_layer_1_attention_output_layernorm_bias_to_fp16, epsilon = const_51_to_fp16, gamma = p_transformer_encoder_layer_1_attention_output_layernorm_weight_to_fp16, x = add_6_cast_fp16)[name = tensor<string, []>("layer_norm_3_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28374272)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29553984)))];
            tensor<fp16, [1, 128, 1536]> linear_10_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_1_intermediate_dense_weight_to_fp16, x = layer_norm_3_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<string, []> gelu_1_mode_0 = const()[name = tensor<string, []>("gelu_1_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_1_cast_fp16 = gelu(mode = gelu_1_mode_0, x = linear_10_cast_fp16)[name = tensor<string, []>("gelu_1_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29557120)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30736832)))];
            tensor<fp16, [1, 128, 384]> linear_11_cast_fp16 = linear(bias = p_transformer_encoder_layer_1_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_1_output_dense_weight_to_fp16, x = gelu_1_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_7_cast_fp16 = add(x = linear_11_cast_fp16, y = layer_norm_3_cast_fp16)[name = tensor<string, []>("add_7_cast_fp16")];
            tensor<int32, [1]> layer_norm_4_axes_0 = const()[name = tensor<string, []>("layer_norm_4_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30737664)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_1_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_1_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30738496)))];
            tensor<fp16, []> const_53_to_fp16 = const()[name = tensor<string, []>("const_53_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_4_cast_fp16 = layer_norm(axes = layer_norm_4_axes_0, beta = p_transformer_encoder_layer_1_output_layernorm_bias_to_fp16, epsilon = const_53_to_fp16, gamma = p_transformer_encoder_layer_1_output_layernorm_weight_to_fp16, x = add_7_cast_fp16)[name = tensor<string, []>("layer_norm_4_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_2_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30739328)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31034304)))];
            tensor<fp16, [1, 128, 384]> linear_12_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_2_attention_self_query_weight_to_fp16, x = layer_norm_4_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_2_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31035136)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31330112)))];
            tensor<fp16, [1, 128, 384]> linear_13_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_2_attention_self_key_weight_to_fp16, x = layer_norm_4_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<int32, [4]> const_54 = const()[name = tensor<string, []>("const_54"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_8_cast_fp16 = reshape(shape = const_54, x = linear_13_cast_fp16)[name = tensor<string, []>("view_8_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_2_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31330944)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31625920)))];
            tensor<fp16, [1, 128, 384]> linear_14_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_2_attention_self_value_weight_to_fp16, x = layer_norm_4_cast_fp16)[name = tensor<string, []>("linear_14_cast_fp16")];
            tensor<int32, [4]> const_56 = const()[name = tensor<string, []>("const_56"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_9_cast_fp16 = reshape(shape = const_56, x = linear_14_cast_fp16)[name = tensor<string, []>("view_9_cast_fp16")];
            tensor<int32, [4]> const_57 = const()[name = tensor<string, []>("const_57"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_58 = const()[name = tensor<string, []>("const_58"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_10_cast_fp16 = reshape(shape = const_58, x = linear_12_cast_fp16)[name = tensor<string, []>("view_10_cast_fp16")];
            tensor<bool, []> matmul_4_transpose_x_0 = const()[name = tensor<string, []>("matmul_4_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_4_transpose_y_0 = const()[name = tensor<string, []>("matmul_4_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_22_perm_0 = const()[name = tensor<string, []>("transpose_22_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_23_perm_0 = const()[name = tensor<string, []>("transpose_23_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_23 = transpose(perm = transpose_23_perm_0, x = view_8_cast_fp16)[name = tensor<string, []>("transpose_43")];
            tensor<fp16, [1, 12, 128, 32]> transpose_22 = transpose(perm = transpose_22_perm_0, x = view_10_cast_fp16)[name = tensor<string, []>("transpose_44")];
            tensor<fp16, [1, 12, 128, 128]> matmul_4_cast_fp16 = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = transpose_22, y = transpose_23)[name = tensor<string, []>("matmul_4_cast_fp16")];
            tensor<fp16, []> _inversed_div_2_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_2_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_2_cast_fp16 = mul(x = matmul_4_cast_fp16, y = _inversed_div_2_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_2_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_8_cast_fp16 = add(x = _inversed_div_2_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_8_cast_fp16")];
            tensor<int32, []> const_63 = const()[name = tensor<string, []>("const_63"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_2_cast_fp16 = softmax(axis = const_63, x = add_8_cast_fp16)[name = tensor<string, []>("softmax_2_cast_fp16")];
            tensor<bool, []> matmul_5_transpose_x_0 = const()[name = tensor<string, []>("matmul_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_5_transpose_y_0 = const()[name = tensor<string, []>("matmul_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_9_cast_fp16 = transpose(perm = const_57, x = view_9_cast_fp16)[name = tensor<string, []>("transpose_45")];
            tensor<fp16, [1, 12, 128, 32]> matmul_5_cast_fp16 = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = softmax_2_cast_fp16, y = permute_9_cast_fp16)[name = tensor<string, []>("matmul_5_cast_fp16")];
            tensor<int32, [4]> const_64 = const()[name = tensor<string, []>("const_64"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_65 = const()[name = tensor<string, []>("const_65"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_11_cast_fp16 = transpose(perm = const_64, x = matmul_5_cast_fp16)[name = tensor<string, []>("transpose_42")];
            tensor<fp16, [1, 128, 384]> view_11_cast_fp16 = reshape(shape = const_65, x = permute_11_cast_fp16)[name = tensor<string, []>("view_11_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_2_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31626752)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31921728)))];
            tensor<fp16, [1, 128, 384]> linear_15_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_2_attention_output_dense_weight_to_fp16, x = view_11_cast_fp16)[name = tensor<string, []>("linear_15_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_9_cast_fp16 = add(x = linear_15_cast_fp16, y = layer_norm_4_cast_fp16)[name = tensor<string, []>("add_9_cast_fp16")];
            tensor<int32, [1]> layer_norm_5_axes_0 = const()[name = tensor<string, []>("layer_norm_5_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31922560)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31923392)))];
            tensor<fp16, []> const_67_to_fp16 = const()[name = tensor<string, []>("const_67_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_5_cast_fp16 = layer_norm(axes = layer_norm_5_axes_0, beta = p_transformer_encoder_layer_2_attention_output_layernorm_bias_to_fp16, epsilon = const_67_to_fp16, gamma = p_transformer_encoder_layer_2_attention_output_layernorm_weight_to_fp16, x = add_9_cast_fp16)[name = tensor<string, []>("layer_norm_5_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_2_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31924224)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33103936)))];
            tensor<fp16, [1, 128, 1536]> linear_16_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_2_intermediate_dense_weight_to_fp16, x = layer_norm_5_cast_fp16)[name = tensor<string, []>("linear_16_cast_fp16")];
            tensor<string, []> gelu_2_mode_0 = const()[name = tensor<string, []>("gelu_2_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_2_cast_fp16 = gelu(mode = gelu_2_mode_0, x = linear_16_cast_fp16)[name = tensor<string, []>("gelu_2_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_2_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33107072)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34286784)))];
            tensor<fp16, [1, 128, 384]> linear_17_cast_fp16 = linear(bias = p_transformer_encoder_layer_2_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_2_output_dense_weight_to_fp16, x = gelu_2_cast_fp16)[name = tensor<string, []>("linear_17_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_10_cast_fp16 = add(x = linear_17_cast_fp16, y = layer_norm_5_cast_fp16)[name = tensor<string, []>("add_10_cast_fp16")];
            tensor<int32, [1]> layer_norm_6_axes_0 = const()[name = tensor<string, []>("layer_norm_6_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34287616)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_2_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_2_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34288448)))];
            tensor<fp16, []> const_69_to_fp16 = const()[name = tensor<string, []>("const_69_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_6_cast_fp16 = layer_norm(axes = layer_norm_6_axes_0, beta = p_transformer_encoder_layer_2_output_layernorm_bias_to_fp16, epsilon = const_69_to_fp16, gamma = p_transformer_encoder_layer_2_output_layernorm_weight_to_fp16, x = add_10_cast_fp16)[name = tensor<string, []>("layer_norm_6_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_3_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34289280)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34584256)))];
            tensor<fp16, [1, 128, 384]> linear_18_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_3_attention_self_query_weight_to_fp16, x = layer_norm_6_cast_fp16)[name = tensor<string, []>("linear_18_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_3_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34585088)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34880064)))];
            tensor<fp16, [1, 128, 384]> linear_19_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_3_attention_self_key_weight_to_fp16, x = layer_norm_6_cast_fp16)[name = tensor<string, []>("linear_19_cast_fp16")];
            tensor<int32, [4]> const_70 = const()[name = tensor<string, []>("const_70"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_12_cast_fp16 = reshape(shape = const_70, x = linear_19_cast_fp16)[name = tensor<string, []>("view_12_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_3_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34880896)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35175872)))];
            tensor<fp16, [1, 128, 384]> linear_20_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_3_attention_self_value_weight_to_fp16, x = layer_norm_6_cast_fp16)[name = tensor<string, []>("linear_20_cast_fp16")];
            tensor<int32, [4]> const_72 = const()[name = tensor<string, []>("const_72"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_13_cast_fp16 = reshape(shape = const_72, x = linear_20_cast_fp16)[name = tensor<string, []>("view_13_cast_fp16")];
            tensor<int32, [4]> const_73 = const()[name = tensor<string, []>("const_73"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_74 = const()[name = tensor<string, []>("const_74"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_14_cast_fp16 = reshape(shape = const_74, x = linear_18_cast_fp16)[name = tensor<string, []>("view_14_cast_fp16")];
            tensor<bool, []> matmul_6_transpose_x_0 = const()[name = tensor<string, []>("matmul_6_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_6_transpose_y_0 = const()[name = tensor<string, []>("matmul_6_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_24_perm_0 = const()[name = tensor<string, []>("transpose_24_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_25_perm_0 = const()[name = tensor<string, []>("transpose_25_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_25 = transpose(perm = transpose_25_perm_0, x = view_12_cast_fp16)[name = tensor<string, []>("transpose_39")];
            tensor<fp16, [1, 12, 128, 32]> transpose_24 = transpose(perm = transpose_24_perm_0, x = view_14_cast_fp16)[name = tensor<string, []>("transpose_40")];
            tensor<fp16, [1, 12, 128, 128]> matmul_6_cast_fp16 = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = transpose_24, y = transpose_25)[name = tensor<string, []>("matmul_6_cast_fp16")];
            tensor<fp16, []> _inversed_div_3_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_3_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_3_cast_fp16 = mul(x = matmul_6_cast_fp16, y = _inversed_div_3_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_3_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_11_cast_fp16 = add(x = _inversed_div_3_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_11_cast_fp16")];
            tensor<int32, []> const_79 = const()[name = tensor<string, []>("const_79"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_3_cast_fp16 = softmax(axis = const_79, x = add_11_cast_fp16)[name = tensor<string, []>("softmax_3_cast_fp16")];
            tensor<bool, []> matmul_7_transpose_x_0 = const()[name = tensor<string, []>("matmul_7_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_7_transpose_y_0 = const()[name = tensor<string, []>("matmul_7_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_13_cast_fp16 = transpose(perm = const_73, x = view_13_cast_fp16)[name = tensor<string, []>("transpose_41")];
            tensor<fp16, [1, 12, 128, 32]> matmul_7_cast_fp16 = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = softmax_3_cast_fp16, y = permute_13_cast_fp16)[name = tensor<string, []>("matmul_7_cast_fp16")];
            tensor<int32, [4]> const_80 = const()[name = tensor<string, []>("const_80"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_81 = const()[name = tensor<string, []>("const_81"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_15_cast_fp16 = transpose(perm = const_80, x = matmul_7_cast_fp16)[name = tensor<string, []>("transpose_38")];
            tensor<fp16, [1, 128, 384]> view_15_cast_fp16 = reshape(shape = const_81, x = permute_15_cast_fp16)[name = tensor<string, []>("view_15_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_3_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35176704)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35471680)))];
            tensor<fp16, [1, 128, 384]> linear_21_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_3_attention_output_dense_weight_to_fp16, x = view_15_cast_fp16)[name = tensor<string, []>("linear_21_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_12_cast_fp16 = add(x = linear_21_cast_fp16, y = layer_norm_6_cast_fp16)[name = tensor<string, []>("add_12_cast_fp16")];
            tensor<int32, [1]> layer_norm_7_axes_0 = const()[name = tensor<string, []>("layer_norm_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35472512)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35473344)))];
            tensor<fp16, []> const_83_to_fp16 = const()[name = tensor<string, []>("const_83_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_7_cast_fp16 = layer_norm(axes = layer_norm_7_axes_0, beta = p_transformer_encoder_layer_3_attention_output_layernorm_bias_to_fp16, epsilon = const_83_to_fp16, gamma = p_transformer_encoder_layer_3_attention_output_layernorm_weight_to_fp16, x = add_12_cast_fp16)[name = tensor<string, []>("layer_norm_7_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_3_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35474176)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36653888)))];
            tensor<fp16, [1, 128, 1536]> linear_22_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_3_intermediate_dense_weight_to_fp16, x = layer_norm_7_cast_fp16)[name = tensor<string, []>("linear_22_cast_fp16")];
            tensor<string, []> gelu_3_mode_0 = const()[name = tensor<string, []>("gelu_3_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_3_cast_fp16 = gelu(mode = gelu_3_mode_0, x = linear_22_cast_fp16)[name = tensor<string, []>("gelu_3_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_3_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36657024)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37836736)))];
            tensor<fp16, [1, 128, 384]> linear_23_cast_fp16 = linear(bias = p_transformer_encoder_layer_3_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_3_output_dense_weight_to_fp16, x = gelu_3_cast_fp16)[name = tensor<string, []>("linear_23_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_13_cast_fp16 = add(x = linear_23_cast_fp16, y = layer_norm_7_cast_fp16)[name = tensor<string, []>("add_13_cast_fp16")];
            tensor<int32, [1]> layer_norm_8_axes_0 = const()[name = tensor<string, []>("layer_norm_8_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37837568)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_3_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_3_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37838400)))];
            tensor<fp16, []> const_85_to_fp16 = const()[name = tensor<string, []>("const_85_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_8_cast_fp16 = layer_norm(axes = layer_norm_8_axes_0, beta = p_transformer_encoder_layer_3_output_layernorm_bias_to_fp16, epsilon = const_85_to_fp16, gamma = p_transformer_encoder_layer_3_output_layernorm_weight_to_fp16, x = add_13_cast_fp16)[name = tensor<string, []>("layer_norm_8_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_4_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37839232)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38134208)))];
            tensor<fp16, [1, 128, 384]> linear_24_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_4_attention_self_query_weight_to_fp16, x = layer_norm_8_cast_fp16)[name = tensor<string, []>("linear_24_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_4_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38135040)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38430016)))];
            tensor<fp16, [1, 128, 384]> linear_25_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_4_attention_self_key_weight_to_fp16, x = layer_norm_8_cast_fp16)[name = tensor<string, []>("linear_25_cast_fp16")];
            tensor<int32, [4]> const_86 = const()[name = tensor<string, []>("const_86"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_16_cast_fp16 = reshape(shape = const_86, x = linear_25_cast_fp16)[name = tensor<string, []>("view_16_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_4_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38430848)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38725824)))];
            tensor<fp16, [1, 128, 384]> linear_26_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_4_attention_self_value_weight_to_fp16, x = layer_norm_8_cast_fp16)[name = tensor<string, []>("linear_26_cast_fp16")];
            tensor<int32, [4]> const_88 = const()[name = tensor<string, []>("const_88"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_17_cast_fp16 = reshape(shape = const_88, x = linear_26_cast_fp16)[name = tensor<string, []>("view_17_cast_fp16")];
            tensor<int32, [4]> const_89 = const()[name = tensor<string, []>("const_89"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_90 = const()[name = tensor<string, []>("const_90"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_18_cast_fp16 = reshape(shape = const_90, x = linear_24_cast_fp16)[name = tensor<string, []>("view_18_cast_fp16")];
            tensor<bool, []> matmul_8_transpose_x_0 = const()[name = tensor<string, []>("matmul_8_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_8_transpose_y_0 = const()[name = tensor<string, []>("matmul_8_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_26_perm_0 = const()[name = tensor<string, []>("transpose_26_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_27_perm_0 = const()[name = tensor<string, []>("transpose_27_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_27 = transpose(perm = transpose_27_perm_0, x = view_16_cast_fp16)[name = tensor<string, []>("transpose_35")];
            tensor<fp16, [1, 12, 128, 32]> transpose_26 = transpose(perm = transpose_26_perm_0, x = view_18_cast_fp16)[name = tensor<string, []>("transpose_36")];
            tensor<fp16, [1, 12, 128, 128]> matmul_8_cast_fp16 = matmul(transpose_x = matmul_8_transpose_x_0, transpose_y = matmul_8_transpose_y_0, x = transpose_26, y = transpose_27)[name = tensor<string, []>("matmul_8_cast_fp16")];
            tensor<fp16, []> _inversed_div_4_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_4_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_4_cast_fp16 = mul(x = matmul_8_cast_fp16, y = _inversed_div_4_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_4_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_14_cast_fp16 = add(x = _inversed_div_4_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_14_cast_fp16")];
            tensor<int32, []> const_95 = const()[name = tensor<string, []>("const_95"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_4_cast_fp16 = softmax(axis = const_95, x = add_14_cast_fp16)[name = tensor<string, []>("softmax_4_cast_fp16")];
            tensor<bool, []> matmul_9_transpose_x_0 = const()[name = tensor<string, []>("matmul_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_9_transpose_y_0 = const()[name = tensor<string, []>("matmul_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_17_cast_fp16 = transpose(perm = const_89, x = view_17_cast_fp16)[name = tensor<string, []>("transpose_37")];
            tensor<fp16, [1, 12, 128, 32]> matmul_9_cast_fp16 = matmul(transpose_x = matmul_9_transpose_x_0, transpose_y = matmul_9_transpose_y_0, x = softmax_4_cast_fp16, y = permute_17_cast_fp16)[name = tensor<string, []>("matmul_9_cast_fp16")];
            tensor<int32, [4]> const_96 = const()[name = tensor<string, []>("const_96"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_97 = const()[name = tensor<string, []>("const_97"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_19_cast_fp16 = transpose(perm = const_96, x = matmul_9_cast_fp16)[name = tensor<string, []>("transpose_34")];
            tensor<fp16, [1, 128, 384]> view_19_cast_fp16 = reshape(shape = const_97, x = permute_19_cast_fp16)[name = tensor<string, []>("view_19_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_4_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38726656)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39021632)))];
            tensor<fp16, [1, 128, 384]> linear_27_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_4_attention_output_dense_weight_to_fp16, x = view_19_cast_fp16)[name = tensor<string, []>("linear_27_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_15_cast_fp16 = add(x = linear_27_cast_fp16, y = layer_norm_8_cast_fp16)[name = tensor<string, []>("add_15_cast_fp16")];
            tensor<int32, [1]> layer_norm_9_axes_0 = const()[name = tensor<string, []>("layer_norm_9_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39022464)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39023296)))];
            tensor<fp16, []> const_99_to_fp16 = const()[name = tensor<string, []>("const_99_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_9_cast_fp16 = layer_norm(axes = layer_norm_9_axes_0, beta = p_transformer_encoder_layer_4_attention_output_layernorm_bias_to_fp16, epsilon = const_99_to_fp16, gamma = p_transformer_encoder_layer_4_attention_output_layernorm_weight_to_fp16, x = add_15_cast_fp16)[name = tensor<string, []>("layer_norm_9_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_4_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39024128)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40203840)))];
            tensor<fp16, [1, 128, 1536]> linear_28_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_4_intermediate_dense_weight_to_fp16, x = layer_norm_9_cast_fp16)[name = tensor<string, []>("linear_28_cast_fp16")];
            tensor<string, []> gelu_4_mode_0 = const()[name = tensor<string, []>("gelu_4_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_4_cast_fp16 = gelu(mode = gelu_4_mode_0, x = linear_28_cast_fp16)[name = tensor<string, []>("gelu_4_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_4_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40206976)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41386688)))];
            tensor<fp16, [1, 128, 384]> linear_29_cast_fp16 = linear(bias = p_transformer_encoder_layer_4_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_4_output_dense_weight_to_fp16, x = gelu_4_cast_fp16)[name = tensor<string, []>("linear_29_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_16_cast_fp16 = add(x = linear_29_cast_fp16, y = layer_norm_9_cast_fp16)[name = tensor<string, []>("add_16_cast_fp16")];
            tensor<int32, [1]> layer_norm_10_axes_0 = const()[name = tensor<string, []>("layer_norm_10_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41387520)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_4_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_4_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41388352)))];
            tensor<fp16, []> const_101_to_fp16 = const()[name = tensor<string, []>("const_101_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_10_cast_fp16 = layer_norm(axes = layer_norm_10_axes_0, beta = p_transformer_encoder_layer_4_output_layernorm_bias_to_fp16, epsilon = const_101_to_fp16, gamma = p_transformer_encoder_layer_4_output_layernorm_weight_to_fp16, x = add_16_cast_fp16)[name = tensor<string, []>("layer_norm_10_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_5_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41389184)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41684160)))];
            tensor<fp16, [1, 128, 384]> linear_30_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_attention_self_query_bias_to_fp16, weight = p_transformer_encoder_layer_5_attention_self_query_weight_to_fp16, x = layer_norm_10_cast_fp16)[name = tensor<string, []>("linear_30_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_5_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41684992)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41979968)))];
            tensor<fp16, [1, 128, 384]> linear_31_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_attention_self_key_bias_to_fp16, weight = p_transformer_encoder_layer_5_attention_self_key_weight_to_fp16, x = layer_norm_10_cast_fp16)[name = tensor<string, []>("linear_31_cast_fp16")];
            tensor<int32, [4]> const_102 = const()[name = tensor<string, []>("const_102"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_20_cast_fp16 = reshape(shape = const_102, x = linear_31_cast_fp16)[name = tensor<string, []>("view_20_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_5_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41980800)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42275776)))];
            tensor<fp16, [1, 128, 384]> linear_32_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_attention_self_value_bias_to_fp16, weight = p_transformer_encoder_layer_5_attention_self_value_weight_to_fp16, x = layer_norm_10_cast_fp16)[name = tensor<string, []>("linear_32_cast_fp16")];
            tensor<int32, [4]> const_104 = const()[name = tensor<string, []>("const_104"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_21_cast_fp16 = reshape(shape = const_104, x = linear_32_cast_fp16)[name = tensor<string, []>("view_21_cast_fp16")];
            tensor<int32, [4]> const_105 = const()[name = tensor<string, []>("const_105"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [4]> const_106 = const()[name = tensor<string, []>("const_106"), val = tensor<int32, [4]>([1, 128, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> view_22_cast_fp16 = reshape(shape = const_106, x = linear_30_cast_fp16)[name = tensor<string, []>("view_22_cast_fp16")];
            tensor<bool, []> matmul_10_transpose_x_0 = const()[name = tensor<string, []>("matmul_10_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_10_transpose_y_0 = const()[name = tensor<string, []>("matmul_10_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_28_perm_0 = const()[name = tensor<string, []>("transpose_28_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_29_perm_0 = const()[name = tensor<string, []>("transpose_29_perm_0"), val = tensor<int32, [4]>([0, 2, -1, -3])];
            tensor<fp16, [1, 12, 32, 128]> transpose_29 = transpose(perm = transpose_29_perm_0, x = view_20_cast_fp16)[name = tensor<string, []>("transpose_31")];
            tensor<fp16, [1, 12, 128, 32]> transpose_28 = transpose(perm = transpose_28_perm_0, x = view_22_cast_fp16)[name = tensor<string, []>("transpose_32")];
            tensor<fp16, [1, 12, 128, 128]> matmul_10_cast_fp16 = matmul(transpose_x = matmul_10_transpose_x_0, transpose_y = matmul_10_transpose_y_0, x = transpose_28, y = transpose_29)[name = tensor<string, []>("matmul_10_cast_fp16")];
            tensor<fp16, []> _inversed_div_5_y_0_to_fp16 = const()[name = tensor<string, []>("_inversed_div_5_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 12, 128, 128]> _inversed_div_5_cast_fp16 = mul(x = matmul_10_cast_fp16, y = _inversed_div_5_y_0_to_fp16)[name = tensor<string, []>("_inversed_div_5_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_17_cast_fp16 = add(x = _inversed_div_5_cast_fp16, y = mul_cast_fp16)[name = tensor<string, []>("add_17_cast_fp16")];
            tensor<int32, []> const_111 = const()[name = tensor<string, []>("const_111"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_5_cast_fp16 = softmax(axis = const_111, x = add_17_cast_fp16)[name = tensor<string, []>("softmax_5_cast_fp16")];
            tensor<bool, []> matmul_11_transpose_x_0 = const()[name = tensor<string, []>("matmul_11_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> matmul_11_transpose_y_0 = const()[name = tensor<string, []>("matmul_11_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> permute_21_cast_fp16 = transpose(perm = const_105, x = view_21_cast_fp16)[name = tensor<string, []>("transpose_33")];
            tensor<fp16, [1, 12, 128, 32]> matmul_11_cast_fp16 = matmul(transpose_x = matmul_11_transpose_x_0, transpose_y = matmul_11_transpose_y_0, x = softmax_5_cast_fp16, y = permute_21_cast_fp16)[name = tensor<string, []>("matmul_11_cast_fp16")];
            tensor<int32, [4]> const_112 = const()[name = tensor<string, []>("const_112"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> const_113 = const()[name = tensor<string, []>("const_113"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> permute_23_cast_fp16 = transpose(perm = const_112, x = matmul_11_cast_fp16)[name = tensor<string, []>("transpose_30")];
            tensor<fp16, [1, 128, 384]> view_23_cast_fp16 = reshape(shape = const_113, x = permute_23_cast_fp16)[name = tensor<string, []>("view_23_cast_fp16")];
            tensor<fp16, [384, 384]> p_transformer_encoder_layer_5_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42276608)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42571584)))];
            tensor<fp16, [1, 128, 384]> linear_33_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_5_attention_output_dense_weight_to_fp16, x = view_23_cast_fp16)[name = tensor<string, []>("linear_33_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_18_cast_fp16 = add(x = linear_33_cast_fp16, y = layer_norm_10_cast_fp16)[name = tensor<string, []>("add_18_cast_fp16")];
            tensor<int32, [1]> layer_norm_11_axes_0 = const()[name = tensor<string, []>("layer_norm_11_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42572416)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_attention_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_attention_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42573248)))];
            tensor<fp16, []> const_115_to_fp16 = const()[name = tensor<string, []>("const_115_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_11_cast_fp16 = layer_norm(axes = layer_norm_11_axes_0, beta = p_transformer_encoder_layer_5_attention_output_layernorm_bias_to_fp16, epsilon = const_115_to_fp16, gamma = p_transformer_encoder_layer_5_attention_output_layernorm_weight_to_fp16, x = add_18_cast_fp16)[name = tensor<string, []>("layer_norm_11_cast_fp16")];
            tensor<fp16, [1536, 384]> p_transformer_encoder_layer_5_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42574080)))];
            tensor<fp16, [1536]> p_transformer_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43753792)))];
            tensor<fp16, [1, 128, 1536]> linear_34_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = p_transformer_encoder_layer_5_intermediate_dense_weight_to_fp16, x = layer_norm_11_cast_fp16)[name = tensor<string, []>("linear_34_cast_fp16")];
            tensor<string, []> gelu_5_mode_0 = const()[name = tensor<string, []>("gelu_5_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> gelu_5_cast_fp16 = gelu(mode = gelu_5_mode_0, x = linear_34_cast_fp16)[name = tensor<string, []>("gelu_5_cast_fp16")];
            tensor<fp16, [384, 1536]> p_transformer_encoder_layer_5_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43756928)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44936640)))];
            tensor<fp16, [1, 128, 384]> linear_35_cast_fp16 = linear(bias = p_transformer_encoder_layer_5_output_dense_bias_to_fp16, weight = p_transformer_encoder_layer_5_output_dense_weight_to_fp16, x = gelu_5_cast_fp16)[name = tensor<string, []>("linear_35_cast_fp16")];
            tensor<fp16, [1, 128, 384]> add_19_cast_fp16 = add(x = linear_35_cast_fp16, y = layer_norm_11_cast_fp16)[name = tensor<string, []>("add_19_cast_fp16")];
            tensor<int32, [1]> layer_norm_12_axes_0 = const()[name = tensor<string, []>("layer_norm_12_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_output_layernorm_weight_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_output_layernorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44937472)))];
            tensor<fp16, [384]> p_transformer_encoder_layer_5_output_layernorm_bias_to_fp16 = const()[name = tensor<string, []>("p_transformer_encoder_layer_5_output_layernorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44938304)))];
            tensor<fp16, []> const_117_to_fp16 = const()[name = tensor<string, []>("const_117_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> layer_norm_12_cast_fp16 = layer_norm(axes = layer_norm_12_axes_0, beta = p_transformer_encoder_layer_5_output_layernorm_bias_to_fp16, epsilon = const_117_to_fp16, gamma = p_transformer_encoder_layer_5_output_layernorm_weight_to_fp16, x = add_19_cast_fp16)[name = tensor<string, []>("layer_norm_12_cast_fp16")];
            tensor<int32, [1]> unsqueeze_2_axes_0 = const()[name = tensor<string, []>("unsqueeze_2_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<int32, [1, 128, 1]> unsqueeze_2 = expand_dims(axes = unsqueeze_2_axes_0, x = attention_mask)[name = tensor<string, []>("unsqueeze_2")];
            tensor<int32, [3]> expand_1_reps_0 = const()[name = tensor<string, []>("expand_1_reps_0"), val = tensor<int32, [3]>([1, 1, 384])];
            tensor<int32, [1, 128, 384]> expand_1 = tile(reps = expand_1_reps_0, x = unsqueeze_2)[name = tensor<string, []>("expand_1")];
            tensor<string, []> _to_copy_1_to_fp16_dtype_0 = const()[name = tensor<string, []>("_to_copy_1_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 128, 384]> expand_1_to_fp16 = cast(dtype = _to_copy_1_to_fp16_dtype_0, x = expand_1)[name = tensor<string, []>("cast_44")];
            tensor<fp16, [1, 128, 384]> mul_1_cast_fp16 = mul(x = layer_norm_12_cast_fp16, y = expand_1_to_fp16)[name = tensor<string, []>("mul_1_cast_fp16")];
            tensor<int32, [1]> sum_1_axes_0 = const()[name = tensor<string, []>("sum_1_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<bool, []> sum_1_keep_dims_0 = const()[name = tensor<string, []>("sum_1_keep_dims_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 384]> sum_1_cast_fp16 = reduce_sum(axes = sum_1_axes_0, keep_dims = sum_1_keep_dims_0, x = mul_1_cast_fp16)[name = tensor<string, []>("sum_1_cast_fp16")];
            tensor<int32, [1]> sum_2_axes_0 = const()[name = tensor<string, []>("sum_2_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<bool, []> sum_2_keep_dims_0 = const()[name = tensor<string, []>("sum_2_keep_dims_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 384]> sum_2_cast_fp16 = reduce_sum(axes = sum_2_axes_0, keep_dims = sum_2_keep_dims_0, x = expand_1_to_fp16)[name = tensor<string, []>("sum_2_cast_fp16")];
            tensor<fp16, []> const_124_to_fp16 = const()[name = tensor<string, []>("const_124_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, []> const_125_to_fp16 = const()[name = tensor<string, []>("const_125_to_fp16"), val = tensor<fp16, []>(inf)];
            tensor<fp16, [1, 384]> clip_0_cast_fp16 = clip(alpha = const_124_to_fp16, beta = const_125_to_fp16, x = sum_2_cast_fp16)[name = tensor<string, []>("clip_0_cast_fp16")];
            tensor<fp16, [1, 384]> div_6_cast_fp16 = real_div(x = sum_1_cast_fp16, y = clip_0_cast_fp16)[name = tensor<string, []>("div_6_cast_fp16")];
            tensor<int32, [1]> const_127 = const()[name = tensor<string, []>("const_127"), val = tensor<int32, [1]>([1])];
            tensor<bool, []> const_128 = const()[name = tensor<string, []>("const_128"), val = tensor<bool, []>(true)];
            tensor<fp16, [1, 1]> linalg_vector_norm_cast_fp16 = reduce_l2_norm(axes = const_127, keep_dims = const_128, x = div_6_cast_fp16)[name = tensor<string, []>("linalg_vector_norm_cast_fp16")];
            tensor<fp16, []> const_129_to_fp16 = const()[name = tensor<string, []>("const_129_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 1]> clamp_min_cast_fp16 = maximum(x = linalg_vector_norm_cast_fp16, y = const_129_to_fp16)[name = tensor<string, []>("clamp_min_cast_fp16")];
            tensor<int32, [2]> expand_2_reps_0 = const()[name = tensor<string, []>("expand_2_reps_0"), val = tensor<int32, [2]>([1, 384])];
            tensor<fp16, [1, 384]> expand_2_cast_fp16 = tile(reps = expand_2_reps_0, x = clamp_min_cast_fp16)[name = tensor<string, []>("expand_2_cast_fp16")];
            tensor<fp16, [1, 384]> embeddings = real_div(x = div_6_cast_fp16, y = expand_2_cast_fp16)[name = tensor<string, []>("div_7_cast_fp16")];
        } -> (embeddings);
}